{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe75319-a77d-469c-a296-e03731d4fd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Agent Evaluation\n",
    "\n",
    "Now that we have our custom DIY Agent with ML tools, we can evaluate how the model is calling our tools and evaluate traces in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "665bb944-03b7-4a9a-bed5-d4056f4e6a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df050b6c-3ce1-4d18-aa2d-5eab18e47c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh uv pip install ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83c446e-8574-42a3-89c6-dfd6d7f2ccb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from well_agent.utils import get_config_path, DotConfig\n",
    "config_path = get_config_path()\n",
    "config = DotConfig('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a877ab5-8cf7-4444-833a-b17a2740c9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Evaluation Dataset\n",
    "\n",
    "You can edit the requests or expected responses in your evaluation dataset and run evaluation as you iterate your agent, leveraging mlflow to track the computed quality metrics.\n",
    "\n",
    "Evaluate your agent with one of our [predefined LLM scorers](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/predefined-judge-scorers), or try adding [custom metrics](https://learn.microsoft.com/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57362396-1be5-4320-8ffc-e84a35e419f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    for question in config.evaluate.questions\n",
    "]\n",
    "\n",
    "input_example = eval_dataset[0]['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57816b07-504a-45c4-9f4d-2da5d43a3438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55f2d86-1e54-4db9-a3bc-f1089dc9c731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is the most common pattern for calling agents from an application - direct API interfaces, facilitated via the mlflow deployment client (this could also be done via the requests package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54659eb4-0550-4f1a-8159-756bbfdae338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "response = get_deploy_client(\"databricks\").predict(\n",
    "    endpoint=f\"agents_{config.catalog}-{config.schema}-{config.agentify.agent_name}\", \n",
    "    inputs=input_example\n",
    "    )\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7114090d-0bb9-4990-9051-de960fc44bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Sample Application\n",
    "This will use our deploy client and get a local copy of the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af8f9994-30bc-43e6-80cb-5441f0391612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from openai import OpenAI\n",
    "from typing import Any\n",
    "from mlflow.entities import Trace\n",
    "from mlflow.genai.scorers import scorer\n",
    "import pandas as pd\n",
    "\n",
    "# Set up MLflow tracking\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "@mlflow.trace\n",
    "def sample_app(messages: list[dict[str, str]]):\n",
    "    print(messages)\n",
    "    response = get_deploy_client(\"databricks\").predict(\n",
    "        endpoint=f\"agents_{config.catalog}-{config.schema}-{config.agentify.agent_name}\", \n",
    "        inputs={'messages':messages}\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e568190-0f3e-40b3-8476-8a89eedab2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The concept of a scorer has gained a lot traction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56af2324-4a99-41e8-98d3-108fe0c4c06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "from databricks import agents\n",
    "from agent import AGENT\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=sample_app,\n",
    "    scorers=[\n",
    "        Guidelines(name=\"calls_tool\", guidelines=\"The response must call a tool\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716a26a1-505e-4ecf-b7cc-3937d1898356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trace Evaluation\n",
    "\n",
    "One of the most powerful features of MLflow is the ability to trace the execution of a model. This allows us to see the exact inputs and outputs of a model, as well as the code that was used to train it. This is especially important for real tasks where an agent is calling an ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f86c194-5c77-4aab-bbbc-9e482679c502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generated_traces = mlflow.search_traces(run_id=eval_results.run_id)\n",
    "trace = generated_traces.iloc[0]['trace']\n",
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e59784a-0775-49a6-81b6-3bc9d31f2c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Make Sure We Are Calling Tools!\n",
    "\n",
    "We saw that guidelines fail. But we can use traces and spans to quantify the number of tool calls in each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9e6423-cabf-4e7b-9a1a-af7093ef7aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trace.search_spans()[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9d39c7-9d78-4093-9087-42f928194c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import Trace, Feedback, SpanType\n",
    "\n",
    "@scorer\n",
    "def tools_called(trace: Trace) -> Feedback:\n",
    "    # Search particular span type from the trace\n",
    "    sp = trace.search_spans()[0]\n",
    "    num_tool_calls = sum([len(x.get('tool_calls',[])) for x in sp.outputs['messages']])\n",
    "    if num_tool_calls > 0:\n",
    "        return Feedback(\n",
    "            value=\"yes\",\n",
    "            rationale=f\"Tools were called\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=\"no\",\n",
    "            rationale=f\"No tools were called\"\n",
    "        )\n",
    "\n",
    "# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\n",
    "span_check_eval_results = mlflow.genai.evaluate(\n",
    "    data=generated_traces,\n",
    "    scorers=[tools_called]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42584023-c147-42b4-89f0-c8e630fceee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Guidelines and Custom Scorers\n",
    "Correctness is great, but we often need much more granular information about how a model is doing. This scorer checks if the total execution time of the trace is within an acceptable range using trace data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2a5bb8-fe39-4b4e-bce0-c515abdc913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import scorer\n",
    "from mlflow.entities import Trace, Feedback, SpanType\n",
    "\n",
    "@scorer\n",
    "def llm_response_time_good(trace: Trace) -> Feedback:\n",
    "    # Search particular span type from the trace\n",
    "    llm_span = trace.search_spans()[0]\n",
    "    response_time = (llm_span.end_time_ns - llm_span.start_time_ns) / 1e9 # second\n",
    "    max_duration = 5.0\n",
    "    if response_time <= max_duration:\n",
    "        return Feedback(\n",
    "            value=\"yes\",\n",
    "            rationale=f\"LLM response time {response_time:.2f}s is within the {max_duration}s limit.\"\n",
    "        )\n",
    "    else:\n",
    "        return Feedback(\n",
    "            value=\"no\",\n",
    "            rationale=f\"LLM response time {response_time:.2f}s exceeds the {max_duration}s limit.\"\n",
    "        )\n",
    "\n",
    "# Evaluate the scorer using the pre-generated traces from the prerequisite code block.\n",
    "span_check_eval_results = mlflow.genai.evaluate(\n",
    "    data=generated_traces,\n",
    "    scorers=[llm_response_time_good]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dcd4305-8dfb-420d-9c3c-1410aa309f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom APIs for applications\n",
    "\n",
    "MlflowClient exposes granular, thread-safe APIs to start and end traces, manage spans, and set span fields. It provides complete control of the trace lifecycle and structure. These APIs are useful when the Fluent APIs are insufficient for your requirements, such as multi-threaded applications and callbacks.\n",
    "\n",
    "These APIs can be called from anywhere and directed back to Databricks or another tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44027bb1-ae41-4801-a20b-3b292c3ad35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.client import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "root_span = mlflow_client.start_trace(\n",
    "  name=\"simple-rag-agent\",\n",
    "  inputs={\n",
    "          \"query\": \"Demo\",\n",
    "          \"model_name\": \"DBRX\",\n",
    "          \"temperature\": 0,\n",
    "          \"max_tokens\": 200\n",
    "         }\n",
    "  )\n",
    "\n",
    "# Retrieve documents that are similar to the query\n",
    "similarity_search_input = dict(query_text=\"demo\", num_results=3)\n",
    "\n",
    "span_ss = mlflow_client.start_span(\n",
    "      \"search\",\n",
    "      # Specify request_id and parent_id to create the span at the right position in the trace\n",
    "        trace_id=root_span.trace_id,\n",
    "        parent_id=root_span.span_id,\n",
    "        inputs=similarity_search_input\n",
    "  )\n",
    "retrieved = [\"Test Result\"]\n",
    "\n",
    "# Explicitly end the span\n",
    "mlflow_client.end_span(root_span.trace_id, span_id=span_ss.span_id, outputs=retrieved)\n",
    "mlflow_client.end_trace(root_span.trace_id, outputs={\"output\": retrieved})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7692790942574644,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6_evaluation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
